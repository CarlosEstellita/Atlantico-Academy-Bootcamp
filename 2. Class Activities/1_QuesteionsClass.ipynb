{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.QuesteionsClass.ipynb",
      "provenance": [],
      "mount_file_id": "1NyvivXRn06KS0tWF3YniFfD-rQFBkKPV",
      "authorship_tag": "ABX9TyPymtXQiFhjjMrk6gXqkoA5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarlosEstellita/Atlantico-Academy-Bootcamp/blob/main/2.%20Class%20Activities/1_QuesteionsClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First Class Questions:"
      ],
      "metadata": {
        "id": "0dSVBYAQj4k6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O m√≥dulos foram importados via Google Drive!"
      ],
      "metadata": {
        "id": "bAZdmaBuEGxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##First Question:"
      ],
      "metadata": {
        "id": "kIJKZXQGO-yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practicing regular expressions: re.split() e re.findall()"
      ],
      "metadata": {
        "id": "yuw59coBuHte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sTl-IWPJiBX",
        "outputId": "480a3e0c-f851-4c98-e365-c34dfbbf124e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', ' Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', '', \"Won't\", 'that', 'be', 'fun?', '', 'I', 'sure', 'think', 'so.', '', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '1', '9']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "my_string = \"Let's write RegEx!  Won't that be fun?  \" \\\n",
        "            \"I sure think so.  Can you find 4 sentences? \" \\\n",
        "            \"Or perhaps, all 19 words?\"\n",
        "\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s\"\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = r\"\\d\"\n",
        "print(re.findall(digits, my_string))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Second Question:"
      ],
      "metadata": {
        "id": "Psk3sZ7DSZnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing with NLTK"
      ],
      "metadata": {
        "id": "GpH54l5nuMXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "from drive.MyDrive.NLP.src.nlp_utils import get_sample_Santo_Graal\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "scene_one = get_sample_Santo_Graal()\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCL10Kmi1aw8",
        "outputId": "3079de0d-1156-4135-9566-bde2eaa8b809"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'forty-three', 'Pendragon', ',', 'master', 'in', 'Camelot', 'is', 'other', 'ridden', 'an', 'that', 'ounce', 'these', 'ratios', 'to', 'needs', 'course', 'through', 'back', 'but', 'minute', 'could', 'A', 'But', 'swallows', '...', 'interested', 'wind', '.', 'What', 'them', 'are', 'who', 'length', 'at', ']', 'weight', 'Not', 'with', \"'ve\", 'snows', 'I', 'In', 'may', 'a', 'England', 'guiding', 'lord', 'Are', 'KING', 'Yes', 'Will', '?', 'he', 'this', 'sovereign', 'Mercea', 'servant', 'We', 'Who', 'court', 'fly', '[', \"'s\", 'question', 'and', 'mean', 'Arthur', 'here', 'climes', 'Whoa', \"'re\", 'south', 'go', 'speak', 'order', 'if', 'Wait', 'simple', 'must', 'every', '--', 'sun', 'tell', 'maintain', 'second', 'zone', 'The', 'carrying', 'five', 'together', 'King', 'Supposing', 'suggesting', 'times', 'agree', 'empty', 'where', 'of', 'So', \"n't\", 'swallow', 'will', \"'m\", '2', 'velocity', 'anyway', 'right', 'ask', 'European', 'temperate', 'trusty', 'non-migratory', 'line', 'why', 'the', 'under', 'carry', 'you', 'No', ':', 'dorsal', '!', 'They', 'get', 'halves', 'there', 'not', 'bangin', 'matter', 'breadth', 'clop', 'Pull', \"'em\", 'wants', 'found', 'from', 'SCENE', 'winter', 'grips', 'search', 'using', 'point', 'coconuts', 'our', 'am', 'grip', 'bring', 'air-speed', 'got', 'martin', 'carried', 'yeah', 'creeper', 'defeator', 'wings', 'African', 'held', 'SOLDIER', 'Uther', 'coconut', 'Court', 'Patsy', \"'\", 'by', 'on', 'You', 'goes', 'or', 'ARTHUR', 'your', 'kingdom', 'Please', 'join', 'maybe', 'Halt', 'beat', 'then', 'two', 'Saxons', 'warmer', 'it', 'all', 'covered', 'bird', 'That', 'be', 'plover', 'Found', 'It', 'husk', '#', 'house', 'its', 'feathers', 'knights', 'Ridden', 'land', 'Listen', 'son', 'one', 'Am', 'have', 'do', 'my', 'Where', 'use', 'horse', 'tropical', 'seek', 'Britons', 'just', 'since', 'yet', 'me', 'Oh', '1', 'Well', 'strand', 'they', 'castle', 'pound', 'migrate', \"'d\", 'strangers', 'does'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Third question:"
      ],
      "metadata": {
        "id": "gQWqJi4YXZJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "regex and re.search()"
      ],
      "metadata": {
        "id": "y-I-4EGvCtdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from drive.MyDrive.NLP.src.nlp_utils import get_sample_Santo_Graal\n",
        "\n",
        "scene_one =  get_sample_Santo_Graal()\n",
        "sentences =  sent_tokenize(scene_one)\n",
        "\n",
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(), match.end())\n",
        "print(match)\n",
        "\n",
        "# Write a regular expression to search for anything in square brackets: pattern1\n",
        "pattern1 = r\"\\[.*\\]\"\n",
        "\n",
        "# Use re.search to find the first text in square brackets\n",
        "print(re.search(pattern1, scene_one))\n",
        "\n",
        "# Find the script notation at the beginning of the fourth sentence and print it\n",
        "pattern2 = r\"[\\w\\s]+:\"\n",
        "print(re.match(pattern2, sentences[3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au7yoIcOXZAh",
        "outputId": "e2035daf-e9b6-42f9-a6a6-ee97669c87f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "581 589\n",
            "<re.Match object; span=(581, 589), match='coconuts'>\n",
            "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
            "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fourth Question:"
      ],
      "metadata": {
        "id": "nrQzpRZ-aARY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regex with tokenization and NLTK"
      ],
      "metadata": {
        "id": "GSQpR8qw9vaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "\n",
        "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
        "from drive.MyDrive.NLP.src.nlp_utils import get_tweets_sample\n",
        "\n",
        "tweets = get_tweets_sample()\n",
        "\n",
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "print(hashtags)\n",
        "\n",
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2 = r\"([#|@]\\w+)\"\n",
        "\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)\n",
        "\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibGYoQf0Z__X",
        "outputId": "0ce777a0-9810-4b86-c2c5-7bdbb4f45e4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#nlp', '#python']\n",
            "['@Atlantico', '#nlp', '#python']\n",
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@Atlantico', ':)', '#nlp', '#python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fifth Question:"
      ],
      "metadata": {
        "id": "ZC0UCknocOlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphic plot"
      ],
      "metadata": {
        "id": "YYKdvC1yC-f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from drive.MyDrive.NLP.src.nlp_utils import get_sample_Santo_Graal\n",
        "\n",
        "# Split the script into lines: lines\n",
        "holy_grail = get_sample_Santo_Graal()\n",
        "lines = holy_grail.split('\\n')\n",
        "\n",
        "# Replace all script lines for speaker\n",
        "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
        "lines = [re.sub(pattern, \" \", l) for l in lines]\n",
        "\n",
        "# Tokenize each line: tokenized_lines\n",
        "tokenized_lines = [regexp_tokenize(s,\"\\w+\") for s in lines]\n",
        "\n",
        "# Make a frequency list of lengths: line_num_words\n",
        "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "# Plot a histogram of the line lengths\n",
        "plt.hist(line_num_words)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "b7k5H1t-cOW_",
        "outputId": "c0ee4ae9-d4f7-4d31-f8d5-c6db783ae0c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOjklEQVR4nO3dfYxldX3H8fenLKgoKdKdUuoyHTRKY40PZGq1WqugZgtGbOIfkNpgSzKJqRZbLV1qUu0fJmitD0kbzVZWSKVYi1iNpC1UMaSJRXdxgYVF8WGrS9FdQowPbUXqt3/cQzuOs3Pv3Htmzvzw/Uomc865Z+b32V9mPnvm3HvuSVUhSWrPTw0dQJI0HQtckhplgUtSoyxwSWqUBS5Jjdq2mYNt3769FhYWNnNISWrevn377q+quZXbN7XAFxYW2Lt372YOKUnNS/Lvq233FIokNcoCl6RGWeCS1CgLXJIaZYFLUqMscElq1NgCT7InyZEkB1Zsf12Su5PcmeTtGxdRkrSaSY7ArwR2Lt+Q5EXA+cAzquqXgHf0H02StJaxBV5VNwMPrNj8GuDyqvp+t8+RDcgmSVrDtFdiPgX4tSRvBf4beGNVfW61HZMsAUsA8/PzUw43rIVd1w8y7qHLzxtkXEltmPZJzG3AKcBzgD8CPpwkq+1YVburarGqFufmfuxSfknSlKYt8MPAdTXyWeCHwPb+YkmSxpm2wP8BeBFAkqcAJwD39xVKkjTe2HPgSa4BXghsT3IYeDOwB9jTvbTwQeCi8u7IkrSpxhZ4VV14jIde1XMWSdI6eCWmJDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRYws8yZ4kR7q776x87A1JKon3w5SkTTbJEfiVwM6VG5OcDrwU+FrPmSRJExhb4FV1M/DAKg+9C7gU8F6YkjSAqc6BJzkfuLeqbus5jyRpQmNvarxSkhOBP2F0+mSS/ZeAJYD5+fn1DidJOoZpjsCfBJwB3JbkELADuDXJz622c1XtrqrFqlqcm5ubPqkk6Ues+wi8qu4Afvbh9a7EF6vq/h5zSZLGmORlhNcAnwHOTHI4ycUbH0uSNM7YI/CqunDM4wu9pZEkTcwrMSWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRk9xSbU+SI0kOLNv250nuTnJ7ko8mOXljY0qSVprkCPxKYOeKbTcCT6uqpwNfBC7rOZckaYyxBV5VNwMPrNh2Q1U91K3+G7BjA7JJktbQxznw3wX+8VgPJllKsjfJ3qNHj/YwnCQJZizwJG8CHgKuPtY+VbW7qharanFubm6W4SRJy2yb9guTvBp4GXBOVVVviSRJE5mqwJPsBC4Ffr2q/rPfSJKkSUzyMsJrgM8AZyY5nORi4C+Bk4Abk+xP8r4NzilJWmHsEXhVXbjK5is2IIskaR28ElOSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNckt1fYkOZLkwLJtpyS5Mck93efHb2xMSdJKkxyBXwnsXLFtF/DJqnoy8MluXZK0icYWeFXdDDywYvP5wFXd8lXAK3rOJUkaY9pz4KdW1X3d8jeAU4+1Y5KlJHuT7D169OiUw0mSVpr5ScyqKqDWeHx3VS1W1eLc3Nysw0mSOtMW+DeTnAbQfT7SXyRJ0iSmLfCPAxd1yxcBH+snjiRpUpO8jPAa4DPAmUkOJ7kYuBx4SZJ7gBd365KkTbRt3A5VdeExHjqn5yySpHXwSkxJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElq1EwFnuQPktyZ5ECSa5I8uq9gkqS1TV3gSZ4A/D6wWFVPA44DLugrmCRpbbOeQtkGPCbJNuBE4D9mjyRJmsTYmxofS1Xdm+QdwNeA/wJuqKobVu6XZAlYApifn592OBZ2XT/110rSI9Esp1AeD5wPnAH8PPDYJK9auV9V7a6qxapanJubmz6pJOlHzHIK5cXAV6vqaFX9ALgO+NV+YkmSxpmlwL8GPCfJiUkCnAMc7CeWJGmcqQu8qm4BrgVuBe7ovtfunnJJksaY+klMgKp6M/DmnrJIktbBKzElqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpUTMVeJKTk1yb5O4kB5M8t69gkqS1zXRLNeA9wD9V1SuTnACc2EMmSdIEpi7wJD8NvAB4NUBVPQg82E8sSdI4sxyBnwEcBT6Q5BnAPuCSqvre8p2SLAFLAPPz8zMM95NnYdf1g4x76PLzBhlX0vrMcg58G3AW8N6qehbwPWDXyp2qandVLVbV4tzc3AzDSZKWm6XADwOHq+qWbv1aRoUuSdoEUxd4VX0D+HqSM7tN5wB39ZJKkjTWrK9CeR1wdfcKlK8AvzN7JEnSJGYq8KraDyz2lEWStA5eiSlJjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNmrnAkxyX5PNJPtFHIEnSZPo4Ar8EONjD95EkrcNMBZ5kB3Ae8P5+4kiSJjXrXenfDVwKnHSsHZIsAUsA8/PzMw6nzbCw6/rBxj50+XmDjS21Zuoj8CQvA45U1b619quq3VW1WFWLc3Nz0w4nSVphllMozwNenuQQ8CHg7CQf7CWVJGmsqQu8qi6rqh1VtQBcAHyqql7VWzJJ0pp8HbgkNWrWJzEBqKpPA5/u43tJkibjEbgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1apa70p+e5KYkdyW5M8klfQaTJK1tlluqPQS8oapuTXISsC/JjVV1V0/ZJElrmOWu9PdV1a3d8neAg8AT+gomSVpbLzc1TrIAPAu4ZZXHloAlgPn5+T6Gk3q3sOv6wcY+dPl5g4z7k/hvfqSZ+UnMJI8DPgK8vqq+vfLxqtpdVYtVtTg3NzfrcJKkzkwFnuR4RuV9dVVd108kSdIkZnkVSoArgINV9c7+IkmSJjHLEfjzgN8Gzk6yv/s4t6dckqQxpn4Ss6r+FUiPWSRJ6+CVmJLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNaqXN7OS+jLkGywN5Sfx3zyUR9obeHkELkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjZr1psY7k3whyZeS7OorlCRpvFluanwc8FfAbwBPBS5M8tS+gkmS1jbLEfizgS9V1Veq6kHgQ8D5/cSSJI0zy5tZPQH4+rL1w8CvrNwpyRKw1K1+N8kXphxvO3D/lF+7kcy1PuZan0dkrrytxyQ/aqvOF3nbTNl+YbWNG/5uhFW1G9g96/dJsreqFnuI1CtzrY+51sdc67NVc8HGZJvlFMq9wOnL1nd02yRJm2CWAv8c8OQkZyQ5AbgA+Hg/sSRJ40x9CqWqHkryWuCfgeOAPVV1Z2/JftzMp2E2iLnWx1zrY6712aq5YAOypar6/p6SpE3glZiS1CgLXJIa1USBb9VL9pMcSnJHkv1J9g6YY0+SI0kOLNt2SpIbk9zTfX78Fsn1liT3dnO2P8m5A+Q6PclNSe5KcmeSS7rtg87ZGrkGnbMkj07y2SS3dbn+rNt+RpJbut/Lv+tezLAVcl2Z5KvL5uuZm5lrWb7jknw+ySe69f7nq6q29AejJ0i/DDwROAG4DXjq0Lm6bIeA7VsgxwuAs4ADy7a9HdjVLe8C3rZFcr0FeOPA83UacFa3fBLwRUZvBzHonK2Ra9A5AwI8rls+HrgFeA7wYeCCbvv7gNdskVxXAq8c8mesy/SHwN8Cn+jWe5+vFo7AvWR/jKq6GXhgxebzgau65auAV2xqKI6Za3BVdV9V3dotfwc4yOjK4kHnbI1cg6qR73arx3cfBZwNXNttH2K+jpVrcEl2AOcB7+/WwwbMVwsFvtol+4P/UHcKuCHJvu4tA7aSU6vqvm75G8CpQ4ZZ4bVJbu9OsWz6qZ3lkiwAz2J09LZl5mxFLhh4zrrTAfuBI8CNjP4q/lZVPdTtMsjv5cpcVfXwfL21m693JXnUZucC3g1cCvywW/8ZNmC+Wijwrez5VXUWo3dk/L0kLxg60Gpq9DfbljgyAd4LPAl4JnAf8BdDBUnyOOAjwOur6tvLHxtyzlbJNficVdX/VNUzGV1x/WzgFzc7w2pW5kryNOAyRvl+GTgF+OPNzJTkZcCRqtq30WO1UOBb9pL9qrq3+3wE+CijH+yt4ptJTgPoPh8ZOA8AVfXN7pfuh8BfM9CcJTmeUUleXVXXdZsHn7PVcm2VOeuyfAu4CXgucHKShy8GHPT3clmund2pqKqq7wMfYPPn63nAy5McYnTK92zgPWzAfLVQ4Fvykv0kj01y0sPLwEuBA2t/1ab6OHBRt3wR8LEBs/yfhwuy85sMMGfd+cgrgINV9c5lDw06Z8fKNfScJZlLcnK3/BjgJYzOz98EvLLbbYj5Wi3X3cv+Ew6j88ybOl9VdVlV7aiqBUZ99amq+i02Yr6GfqZ2wmdzz2X0jPyXgTcNnafL9ERGr4i5DbhzyFzANYz+tP4Bo3NrFzM65/ZJ4B7gX4BTtkiuvwHuAG5nVJinDZDr+YxOj9wO7O8+zh16ztbINeicAU8HPt+NfwD40277E4HPAl8C/h541BbJ9aluvg4AH6R7pcoQH8AL+f9XofQ+X15KL0mNauEUiiRpFRa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatT/AuvZ2tOUOCMyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}